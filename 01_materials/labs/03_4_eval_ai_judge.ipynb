{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe851e5",
   "metadata": {},
   "source": [
    "# AI as Judge\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM as a judge to evaluate LLM outputs. The evaluation can be based on any criteria. G-Eval is implemented by a library called [DeepEval](https://deepeval.com/) which includes a broader set of tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0650f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "document_folder = \"../../05_src/documents/\"\n",
    "blue_cross_file = \"the_blue_cross.txt\"\n",
    "file_path = os.path.join(document_folder, blue_cross_file)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    blue_cross_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843da553",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that summarizes works of fiction with a quirky and bubbly approach.\"\n",
    "PROMPT = \"\"\"\n",
    "    Summarize the following story in at most four paragraphs. Please include all key characters and plot points.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    In addition to the summary, add an introduction paragraph where you greet the reader and a conclusion where you share an opinion about the story.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f951529",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=1.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269743e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae503b",
   "metadata": {},
   "source": [
    "# Answer Relevancy\n",
    "\n",
    "The answer relevancy metric evaluates how relevant the actual output of the LLM app is compared to the provided input. This metric is self-explaining in the sense that the output includes a reason for the metric score.\n",
    "\n",
    "The metric is calculated as:\n",
    "\n",
    "$$\n",
    "AnswerRelevancy=\\frac{NumberRelevantStatements}{TotalStatements}\n",
    "$$\n",
    "\n",
    "Reference: [Answer Relevancy](https://deepeval.com/docs/metrics-answer-relevancy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7de7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.measure(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric.score,metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa92303",
   "metadata": {},
   "source": [
    "# Other Metrics\n",
    "\n",
    "Other useful metric functions include:\n",
    "\n",
    "+ [Faithfulness](https://deepeval.com/docs/metrics-faithfulness): evaluates whether the `actual_output` factually aligns with the contents of  `retrieval_context`. \n",
    "+ [Contextual Precision](https://deepeval.com/docs/metrics-contextual-precision): evaluates whether nodes in your `retrieval_context` that are relevant to the given input are ranked higher than irrelevant ones. \n",
    "+ [Contextual Recall](https://deepeval.com/docs/metrics-contextual-recall): evaluates the extent of which the retrieval_context aligns with the expected_output. \n",
    "+ [Contextual Relevancy](https://deepeval.com/docs/metrics-contextual-relevancy): evaluates the overall relevance of the information presented in your retrieval_context for a given input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7b6ba",
   "metadata": {},
   "source": [
    "# G-Eval\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM-as-a-judge with chain-of-thoughts (CoT) to evaluate LLM outputs based on ANY custom criteria. The G-Eval metric is the most versatile type of metric deepeval offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that specializes in works of fiction.\"\n",
    "PROMPT = \"\"\"\n",
    "    Based on the story below, answer the question provided.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    <question>\n",
    "    Who is the main antagonist in the story and what motivates their actions?\n",
    "    </question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df04e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0eb8ad",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "The most straightforward way to establish a metric is by using a single criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a310a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the context.\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0312796",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9542714",
   "metadata": {},
   "source": [
    "## Evaluation Steps \n",
    "\n",
    "G-Eval is flexible in many ways: notice that we can establish an evaluation criteria or a set of evaluation steps, that can help in guiding the model to follow specific steps to perform the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8647c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'input'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are not OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e44fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
